{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_documents():\n",
    "    documents = []\n",
    "    files = ['docs/linkedin_profile.pdf', 'docs/biography.pdf']\n",
    "    \n",
    "    for file in files:\n",
    "        if os.path.exists(file):\n",
    "            loader = PyPDFLoader(file)\n",
    "            documents.extend(loader.load())\n",
    "        else:\n",
    "            print(f'Warning: {file} not found, skipping...')\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking documents for better retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000, chunk_overlap=200\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Generate Embeddings and Create FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def create_faiss_index(docs, embedding_model='all-MiniLM-L6-v2'):\n",
    "    try:\n",
    "        print('create_faiss_index: downloading embedding model if not available...')\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        vector_db = FAISS.from_documents(docs, embeddings)\n",
    "        vector_db.save_local('vector-store/faiss_index')\n",
    "        \n",
    "        print('FAISS index created successfully.')\n",
    "        return vector_db\n",
    "    except Exception as e:\n",
    "        print(f'unable to create FAISS index: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Mistral-7B (Q4_K_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "def load_llm(model_name='mistral-7b-instruct-v0.1.Q4_K_M.gguf'):\n",
    "    model_path = os.path.join('models', model_name)\n",
    "    if not os.path.exists(model_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        return Llama(model_path=model_path, n_gpu_layers=30, n_threads=8, n_ctx=1024)\n",
    "    except Exception as e:\n",
    "        print(f'Error loading mistral model: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Retrieval-Augmented Generation (RAG) with improved prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, vector_db, llm):\n",
    "    if vector_db is None:\n",
    "        return 'rag_pipeline: No vector database available.'\n",
    "    \n",
    "    retrieved_docs = vector_db.similarity_search(query, k=4)\n",
    "    source_documents = [doc.metadata for doc in retrieved_docs]\n",
    "    context = '\\n'.join([doc.page_content for doc in retrieved_docs])\n",
    "    \n",
    "    prompt = f'''\n",
    "    You are an AI assistant designed to answer questions about Kaung SiThu. \n",
    "    Be gentle, informative, and concise. If you don't know an answer, politely say no.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    User Question: {query}\n",
    "    '''\n",
    "    \n",
    "    response = llm(prompt)\n",
    "    return {\n",
    "        'answer': response['choices'][0]['text'].strip(),\n",
    "        'sources': source_documents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate retrieval and generation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models():\n",
    "    print('evaluate_model: evaluating retriever model (FAISS)...')\n",
    "    try:\n",
    "        faiss_index = FAISS.load_local(\n",
    "            'faiss_index', \n",
    "            HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2'), \n",
    "            allow_dangerous_deserialization=True\n",
    "            )\n",
    "        print(f'evaluate_model: FAISS index successfully loaded and functional.\\n {faiss_index}')\n",
    "    except Exception as e:\n",
    "        print(f'evaluate_model: failed to load FAISS index: {e}')\n",
    "    \n",
    "    print('evaluate_model: evaluating generator model mistral...')\n",
    "    try:\n",
    "        llm = load_llm()    \n",
    "        response = llm('evaluate_model: test query: What is machine learning?')\n",
    "        print(f'evaluate_model: {response}')\n",
    "    except Exception as e:\n",
    "        print(f'evaluate_model: error with mistral model inference: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"Contact\\n+959779056197 (Mobile)\\nneucleyon@gmail.com\\nwww.linkedin.com/in/kaung-\\nsithu-634ab2160 (LinkedIn)\\nwww.facebook.com/archx64/\\n(Personal)\\nTop Skills\\nData Science\\nArtificial Intelligence (AI)\\nNeural Networks\\nCertifications\\nMachine Learning by Stanford\\nUniversity & DeepLearning.AI on\\nCoursera\\nSpring Framework\\nSupervised Machine Learning:\\nRegression and Classification \\nAdvanced Learning Algorithms\\nUnsupervised Learning,\\nRecommenders, Reinforcement\\nLearning\\nKaung SiThu\\nData-Driven Problem Solver\\nYangon, Myanmar\\nSummary\\nI'm just an ordinary guy who is enthusiastic on technology and\\nscience\\nExperience\\nEngineerforce\\nPython Developer\\nMay 2023\\xa0-\\xa0November 2023\\xa0(7 months)\\nTokyo, Japan\\nMakeGood Co.Ltd\\nSenior Developer\\nFebruary 2018\\xa0-\\xa0January 2023\\xa0(5 years)\\nMyanmar\\nEducation\\nWest Yangon Technological University\\nBachelor of Engineering - BE,\\xa0Information Technology\\xa0·\\xa0(2013\\xa0-\\xa02020)\\nAsian Institute of Technology\\nMaster of Engineering - MEng,\\xa0Data Science and Artificial\\nIntelligence\\xa0·\\xa0(August 2024\\xa0-\\xa0May 2026)\\n\\xa0 Page 1 of 1\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Kaung Sithu is a data -driven problem solver with a strong background in data science, \\nartificial intelligence (AI), and software development. Passionate about technology and \\nscience, he has developed expertise in machine learning, neural networks, and software \\nengineering. \\nHe holds a Bachelor of Engineering (BE) in Information Technology from West Yangon \\nTechnological University (2013-2020). To further advance his knowledge, he is currently \\npursuing a Master of Engineering (MEng) in Data Science and Artificial Intelligence at the \\nAsian Institute of Technology (August 2024 - May 2026). \\nProfessionally, Kaung has over five years of experience in software development. He \\nworked as a Senior Developer at MakeGood Co. Ltd. in Myanmar for five years, where he \\ncontributed to various development projects. Later, he worked as a Python Developer at \\nEngineerforce in Tokyo, Japan, for seven months, gaining international exposure and \\nrefining his technical skills. \\nBeyond formal education and work experience, he has strengthened his expertise \\nthrough industry -recognized certifications, including Stanford University & \\nDeepLearning.AI’s Machine Learning course and advanced topics in supervised, \\nunsupervised, and reinforcement learning. \\nKaung Sithu remains committed to innovation and continuous learning in AI and software \\ndevelopment. His technical expertise, combined with a passion for problem -solving, \\nmakes him a valuable contributor in the field of data science and AI-driven technologies.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('loading documents')\n",
    "documents = load_documents()\n",
    "docs = chunk_documents(documents)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of documents: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'size of documents: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS index...\n",
      "create_faiss_index: downloading embedding model if not available...\n",
      "FAISS index created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x28158f7c6e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Creating FAISS index...')\n",
    "vector_db = create_faiss_index(docs)\n",
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models\\mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 1024\n",
      "llama_init_from_model: n_ctx_per_seq = 1024\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_init_from_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    98.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_model: evaluating retriever model (FAISS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models\\mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_model: failed to load FAISS index: Error in __cdecl faiss::FileIOReader::FileIOReader(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:68: Error: 'f' failed: could not open faiss_index\\index.faiss for reading: No such file or directory\n",
      "evaluate_model: evaluating generator model mistral...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 1024\n",
      "llama_init_from_model: n_ctx_per_seq = 1024\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 1024, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_init_from_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    98.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "llama_perf_context_print:        load time =    1203.91 ms\n",
      "llama_perf_context_print: prompt eval time =    1203.81 ms /    13 tokens (   92.60 ms per token,    10.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1204.74 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_model: {'id': 'cmpl-ea5892aa-f5be-424f-8fe8-698fcc60911a', 'object': 'text_completion', 'created': 1741883998, 'model': 'models\\\\mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'choices': [{'text': '', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 0, 'total_tokens': 13}}\n"
     ]
    }
   ],
   "source": [
    "llm = load_llm()\n",
    "evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =   54535.54 ms /   804 tokens (   67.83 ms per token,    14.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2606.61 ms /    15 runs   (  173.77 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   57151.48 ms /   819 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How old is Kaung Sithu?\n",
      "Response: AI Assistant: I'm sorry, but I don't have\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1196.60 ms /    16 tokens (   74.79 ms per token,    13.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2581.08 ms /    15 runs   (  172.07 ms per token,     5.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    3787.88 ms /    31 tokens\n",
      "Llama.generate: 794 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is Kaung Sithu's highest level of education?\n",
      "Response: AI Assistant: Kaung Sithu holds a Bachelor of Engineering (\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1090.86 ms /    15 tokens (   72.72 ms per token,    13.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2593.89 ms /    15 runs   (  172.93 ms per token,     5.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    3694.54 ms /    30 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 12 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What major or field of study did Kaung pursue during your education?\n",
      "Response: AI: Kaung pursued a Bachelor of Engineering (BE) in\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =     887.68 ms /    12 tokens (   73.97 ms per token,    13.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2545.50 ms /    15 runs   (  169.70 ms per token,     5.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    3443.15 ms /    27 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many years of work experience does Kaung have\n",
      "Response: Answer: Kaung has five years of work experience. He worked as\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1104.62 ms /    15 tokens (   73.64 ms per token,    13.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2545.35 ms /    15 runs   (  169.69 ms per token,     5.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    3658.48 ms /    30 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What type of work or industry has Kaung been involved in?\n",
      "Response: Answer: Kaung has been involved in the software development industry and has\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1165.58 ms /    16 tokens (   72.85 ms per token,    13.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2528.89 ms /    15 runs   (  168.59 ms per token,     5.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3704.54 ms /    31 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 20 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Can you describe Kaung's current role or job responsibilities?\n",
      "Response: Answer: Kaung is currently pursuing a Master of Engineering in Data\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1452.65 ms /    20 tokens (   72.63 ms per token,    13.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2527.81 ms /    15 runs   (  168.52 ms per token,     5.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3990.58 ms /    35 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are Kaung's core beliefs regarding the role of technology in shaping society?\n",
      "Response: Answer: Kaung SiThu believes that technology has the potential to\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1107.03 ms /    15 tokens (   73.80 ms per token,    13.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2535.64 ms /    15 runs   (  169.04 ms per token,     5.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    3651.40 ms /    30 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 21 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How does Kaung think cultural values should influence technological advancements?\n",
      "Response: Answer: Kaung SiThu believes that cultural values play a crucial\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1495.45 ms /    21 tokens (   71.21 ms per token,    14.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2608.47 ms /    15 runs   (  173.90 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    4113.40 ms /    36 tokens\n",
      "Llama.generate: 793 prefix-match hit, remaining 25 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: As a master’s student, what is the most challenging aspect of his studies so far?\n",
      "Response: AI Assistant: As a master’s student, Kaung SiTh\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   54536.49 ms\n",
      "llama_perf_context_print: prompt eval time =    1820.89 ms /    25 tokens (   72.84 ms per token,    13.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2880.03 ms /    15 runs   (  192.00 ms per token,     5.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    4709.57 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What specific research interests or academic goals does Kaung hope to achieve during your time as a master’s student?\n",
      "Response: Answer: Kaung Sithu is currently pursuing a Master of\n",
      "Sources: [{'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-03-12T23:10:36+07:00', 'author': 'Kaung Sithu', 'moddate': '2025-03-12T23:10:36+07:00', 'source': 'docs/biography.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, {'producer': 'Apache FOP Version 2.2', 'creator': 'PyPDF', 'creationdate': '2025-03-10T15:50:07+00:00', 'title': 'Resume', 'author': 'LinkedIn', 'subject': 'Resume generated from profile', 'source': 'docs/linkedin_profile.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [   \n",
    "    'How old is Kaung Sithu?',\n",
    "    'What is Kaung Sithu\\'s highest level of education?',\n",
    "    'What major or field of study did Kaung pursue during your education?',\n",
    "    'How many years of work experience does Kaung have',\n",
    "    'What type of work or industry has Kaung been involved in?',\n",
    "    'Can you describe Kaung\\'s current role or job responsibilities?',\n",
    "    'What are Kaung\\'s core beliefs regarding the role of technology in shaping society?',\n",
    "    'How does Kaung think cultural values should influence technological advancements?',\n",
    "    'As a master’s student, what is the most challenging aspect of his studies so far?',\n",
    "    'What specific research interests or academic goals does Kaung hope to achieve during your time as a master’s student?'\n",
    "    ]\n",
    "answers = []\n",
    "for q in questions:\n",
    "    result = rag_pipeline(q, vector_db, llm)\n",
    "    answers.append({'question': q, 'answer': result['answer'], 'sources': result['sources']})\n",
    "    print(f'Q: {q}\\nResponse: {result['answer']}\\nSources: {result['sources']}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda-12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
